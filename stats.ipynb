{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "independent-enzyme",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "checked-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/learner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "import ast\n",
    "import pandas as pd\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-mailing",
   "metadata": {},
   "source": [
    "# Get filename from entered title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "portuguese-jefferson",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a938aed515e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print(get_filename_from_title(\"War and Peace\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mTitle_entered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter title: \"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filename_from_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTitle_entered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#print(filename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             )\n\u001b[0;32m--> 857\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_filename_from_title(title):\n",
    "    File_100_Books = open(\"books_dictionary.txt\", \"r\")\n",
    "    Contents_100_Books = File_100_Books.read()\n",
    "    Dictionary_100_Books = ast.literal_eval(Contents_100_Books)\n",
    "    File_100_Books.close()\n",
    "    #print(Dictionary_100_Books[2]['Title'])\n",
    "    for i in range(len(Dictionary_100_Books)):\n",
    "        if Dictionary_100_Books[i]['Title'] == title:\n",
    "            return Dictionary_100_Books[i]['Filename']\n",
    "\n",
    "\n",
    "#print(get_filename_from_title(\"War and Peace\"))\n",
    "Title_entered = input(\"Enter title: \" )\n",
    "filename = get_filename_from_title(Title_entered)\n",
    "#print(filename)\n",
    "\n",
    "#file = open(\"test.txt\", \"r\")\n",
    "if filename is not None:\n",
    "    file = open(filename, \"r\")\n",
    "else:\n",
    "    print(\"Title was not found.\")\n",
    "    exit()\n",
    "    \n",
    "data = file.read()\n",
    "data_wout_symbols = re.sub(r'[^\\w]', ' ', data) #remove symbols\n",
    "words = data_wout_symbols.split() #split text at space\n",
    "sentences = tokenize.sent_tokenize(data)\n",
    "        \n",
    "def get_list_words_from_title():\n",
    "    #print(get_filename_from_title(\"War and Peace\"))\n",
    "    Title_entered = input(\"Enter title: \" )\n",
    "    filename = get_filename_from_title(Title_entered)\n",
    "    #print(filename)\n",
    "    #file = open(\"test.txt\", \"r\")\n",
    "    if filename is not None:\n",
    "        file = open(filename, \"r\")\n",
    "    else:\n",
    "        print(\"Title was not found.\")\n",
    "        exit()\n",
    "    data = file.read()\n",
    "    data_wout_symbols = re.sub(r'[^\\w]', ' ', data) #remove symbols\n",
    "    words = data_wout_symbols.split() #split text at space\n",
    "    return data_wout_symbols, words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-carbon",
   "metadata": {},
   "source": [
    "# Get list of words, count how often they appear and plot rank v frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "progressive-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN data_wout_symbols, words = get_list_words_from_title()\n",
    "\n",
    "# for i in range(len(words)): \n",
    "#     #make all words lowercase\n",
    "#     words[i] = words[i].lower()\n",
    "\n",
    "# Count_of_letter = collections.Counter(data_wout_symbols) #count how often each letter occurs\n",
    "#END Count_of_words = collections.Counter(words) #count how often each word occurs\n",
    "\n",
    "#print(words) #Print all words\n",
    "#print('Number of words:', len(words)) #How many words are there in total in the txt file\n",
    "Count_of_letter = collections.Counter(data_wout_symbols) #count how often each letter occurs\n",
    "Count_of_words = collections.Counter(words) #count how often each word occurs\n",
    "avg_sen_length = len(sentences)/len(words)\n",
    "\n",
    "#print(frequency_letter) #print all letters and how oftern they occur\n",
    "#print(Count_of_words.most_common(5)) #print x most common words\n",
    "#print(Count_of_words.most_common()[-1]) #print word at the end of the list\n",
    "#print(Count_of_words['war'],Count_of_words['peace']) #print frequency of given word\n",
    "\n",
    "def Rank_Frequency_Plot(N):\n",
    "    \"\"\"\n",
    "    Plot of rank v frequency of N most common words in text\n",
    "    \"\"\"\n",
    "    Most_common_words_decreasing = collections.OrderedDict(Count_of_words.most_common(N)) #most_common returns list and we convert it back into an ordered dictionary for the diagram to work\n",
    "    Frequency_common_words = [(i, Count_of_words[i] / len(words) * 100.0) for i,count in Count_of_words.most_common(N)] #Compute with what percentage the words occur\n",
    "    Frequency_common_words_decreasing = collections.OrderedDict(Frequency_common_words) #Make ordered dictionary of most common words and with which percentage they occur\n",
    "    #print(frequency_percentage[:5]) #print the five most common words with percentage \n",
    "\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.set_xscale('log')\n",
    "    axes.set_yscale('log')\n",
    "    axes.set_xlabel(r'Rank', size=15)\n",
    "    axes.set_ylabel(r'Frequency', size=15)\n",
    "    axes.plot(np.arange(0,N,1), Frequency_common_words_decreasing.values())\n",
    "    #list(frequency_percentage_decreasing).index('is') #Position of a given word in list of most common words\n",
    "    #plt.bar(frequency_percentage_decreasing.keys(), frequency_percentage_decreasing.values())\n",
    "    plt.savefig('diagram.png')\n",
    "    \n",
    "#!!!Rank_Frequency_Plot(3000)\n",
    "#Rank_Frequency_Plot(len(Count_of_words))\n",
    "#print(len(Count_of_words)/len(words)) #Number of unique words/Number total words\n",
    "#print('Number of sentences:', len(sentences))\n",
    "\n",
    "\n",
    "Books_dataframe = pd.read_csv(\"Books_Dataframe.csv\") \n",
    "#print(Books_dataframe)\n",
    "for i in range(100):\n",
    "    print(Books_dataframe.iloc[i,5])\n",
    "    \n",
    "\n",
    "#Add all the numbers to the existing dataframe by first storing them in lists and then adding the lists as new columns to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-jonathan",
   "metadata": {},
   "source": [
    "# Calculate average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fifteen-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c658168666d64b3abdb41af4ca892778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read a .csv file where a pandas dataframe with all the book info is stored.\n",
    "Books_dataframe = pd.read_csv(\"Books_Dataframe.csv\") \n",
    "\n",
    "#Get a list of all the book filenames\n",
    "book_filenames = Books_dataframe[\"Filename\"]\n",
    "\n",
    "#Get a list of all the book titles\n",
    "book_titles = Books_dataframe[\"Title\"]\n",
    "\n",
    "#Create an empty list to which we append the average sentence length of each book\n",
    "avg_sentence_length = []\n",
    "\n",
    "#Loop through all the books\n",
    "for filename in book_filenames:\n",
    "    \n",
    "    #Try to load the file, if it does not exist add an average sentence length of 0 and continue\n",
    "    try:\n",
    "        file = open(filename, \"r\") #Open file\n",
    "    except FileNotFoundError:\n",
    "        avg_sentence_length.append(0) #Set average sentence length to 0 if file does not exist\n",
    "        continue\n",
    "    \n",
    "    #Obtain file content as string\n",
    "    data = file.read()\n",
    "    \n",
    "    #Obtain all the sentences using tlk tokenize\n",
    "    sentences = tokenize.sent_tokenize(data)\n",
    "    \n",
    "    #Obtain all the individual words\n",
    "    data_wout_symbols = re.sub(r'[^\\w]', ' ', data) #remove symbols\n",
    "    words = data_wout_symbols.split() #split text at space\n",
    "    \n",
    "    avg_sentence_length.append(len(words)/len(sentences))\n",
    "\n",
    "avg_sentence_length_sorted = np.sort(avg_sentence_length)    \n",
    "book_titles_sorted = book_titles[np.argsort(avg_sentence_length)]\n",
    "short_titles = [t[:30] for t in book_titles_sorted]\n",
    "    \n",
    "fig, ax = plt.subplots(1, figsize=(12,20))\n",
    "ax.barh(short_titles, avg_sentence_length_sorted, align='center', height=0.8)\n",
    "ax.set_xlabel('Average number of words per sentence')\n",
    "# plt.xticks(rotation = 'vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-magic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
